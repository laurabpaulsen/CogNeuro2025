{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton notebook for subject-level and group-level statistics\n",
    "Use this notebook to write the code for subject- and group-level statistics on EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python\n",
    "First of all, we need to make sure that we are working in the `env` environment.\n",
    "\n",
    "\n",
    "1. Run `bash env_to_jupyter.sh` from the `EEG` folder if you have not already done so. This will make sure that the `env` environment is available as a kernel in this notebook.\n",
    "\n",
    "2. Press `Select Kernel`, then `Jupyter kernel...` and select `env`. If `env` does not show up, press the little refresh symbol!\n",
    "\n",
    "**Note:** You might have to install the Jupyter extension for VScode to be able to select the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowed mean\n",
    "\n",
    "## Subject-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "data_path = Path(\"\") # insert path to the preprocessed epochs of the FaceWord EEG data\n",
    "filename = \"\" # and the filename \n",
    "\n",
    "epochs = mne.read_epochs(data_path / filename, verbose=False, preload=True)\n",
    "\n",
    "# only keep eeg channels\n",
    "epochs.pick([\"eeg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to go the `04-windowed_mean_subject_lvl.ipynb` to get inspiration for how to do a subject-level windowed mean analysis! Add code chunks below as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for subject-level windowed mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-level\n",
    "\n",
    "The first step is to preprocess the data from all subject. I have set up a loop for doing so and defined a few varibles you may need. **Talk through the code in your groups!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used a different script to run FaceWord experiment on Monday where triggers 101 \n",
    "\n",
    "# defining the events\n",
    "event_id_monday_recording = {\n",
    "    'Word/wPos': 11, # positive word\n",
    "    'Wait/wPos': 31, # waiting time after positive word \n",
    "    'Image/wPos': 21, # positive image (always following pos word) \n",
    "    'Word/wNeg': 12, # negative word\n",
    "    'Wait/wNeg': 32, # waiting time after negative word \n",
    "    'Image/wNeg': 22, # negative image (always following neg word) \n",
    "    'Word/wNeu': 13, # neutral word\n",
    "    'Wait/wNeu/iPos': 51, # wait time after neu w (before pos i) \n",
    "    'Image/wNeu/iPos': 41, # positive image (after neu word) \n",
    "    'Wait/wNeu/iNeg': 52, # wait time after neu w (before neg i) \n",
    "    'Image/wNeu/iNeg': 42, # negative image (after neu word) \n",
    "    'Correct/wPos': 201, # correct response ('b') to pos w + image \n",
    "    'Correct/wNeg': 102, # correct response ('y') to neg w + image \n",
    "    'Correct/wNeu/iPos': 111, # cor resp ('b') to neu w + pos image \n",
    "    'Correct/wNeu/iNeg': 112, # cor resp ('y') to neu w + neg image \n",
    "    'Incorrect/wPos': 202, # incor resp ('y') to pos w + image \n",
    "    'Incorrect/wNeg': 101, # incor resp ('b') to neg w + image \n",
    "    'Incorrect/wNeu/iPos': 212, # incor resp ('y') to neu w + pos i \n",
    "    'Incorrect/Neu/iNeg': 211 # incor resp ('b') to neu w + neg i\n",
    "}\n",
    "\n",
    "event_id = {\n",
    "    'Word/wPos': 11, # positive word\n",
    "    'Wait/wPos': 31, # waiting time after positive word \n",
    "    'Image/wPos': 21, # positive image (always following pos word) \n",
    "    'Word/wNeg': 12, # negative word\n",
    "    'Wait/wNeg': 32, # waiting time after negative word \n",
    "    'Image/wNeg': 22, # negative image (always following neg word) \n",
    "    'Word/wNeu': 13, # neutral word\n",
    "    'Wait/wNeu/iPos': 51, # wait time after neu w (before pos i) \n",
    "    'Image/wNeu/iPos': 41, # positive image (after neu word) \n",
    "    'Wait/wNeu/iNeg': 52, # wait time after neu w (before neg i) \n",
    "    'Image/wNeu/iNeg': 42, # negative image (after neu word) \n",
    "    'Correct/wPos': 101, # correct response ('b') to pos w + image \n",
    "    'Correct/wNeg': 102, # correct response ('y') to neg w + image \n",
    "    'Correct/wNeu/iPos': 111, # cor resp ('b') to neu w + pos image \n",
    "    'Correct/wNeu/iNeg': 112, # cor resp ('y') to neu w + neg image \n",
    "    'Incorrect/wPos': 202, # incor resp ('y') to pos w + image \n",
    "    'Incorrect/wNeg': 201, # incor resp ('b') to neg w + image \n",
    "    'Incorrect/wNeu/iPos': 212, # incor resp ('y') to neu w + pos i \n",
    "    'Incorrect/Neu/iNeg': 211 # incor resp ('b') to neu w + neg i\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_EEG_data(raw, bad_channels:list[str], event_id, tmin = -0.1, tmax = 0.7, baseline = (None, 0), l_freq = 0.1, h_freq = 40.0, reject = {\"eeg\": 150e-6}):\n",
    "\n",
    "    raw.info[\"bads\"] = bad_channels\n",
    "\n",
    "\n",
    "    # setting EOG channels correctly - different names for the channels on the two different acquistion computers\n",
    "    try:\n",
    "        raw.set_channel_types({\"HEOG\": \"eog\", \"VEOG\": \"eog\"})\n",
    "    \n",
    "    except(ValueError):\n",
    "        raw.set_channel_types({\"EOG1\": \"eog\", \"EOG2\": \"eog\"})\n",
    "\n",
    "    raw.set_montage(\"standard_1020\")\n",
    "\n",
    "    \n",
    "\n",
    "    # interpolate the bad channels \n",
    "    # YOU COULD ALSO FIND THE COMMON GOOD CHANNELS WHEN DOING STATS INSTEAD\n",
    "    # -> BUT WE WILL DO THIS FOR TODAY\n",
    "    #raw.interpolate_bads(reset_bads=True, mode=\"accurate\", verbose=False)\n",
    "\n",
    "    # dropping the remaining bad channels after interpolation (Fp1 and Fp2)\n",
    "    raw.drop_channels(raw.info[\"bads\"])\n",
    "\n",
    "    raw.filter(l_freq, h_freq, verbose=False)\n",
    "    \n",
    "    # setting the reference\n",
    "    raw.set_eeg_reference(\"average\", projection=False, verbose = False)\n",
    "\n",
    "    # creating the events\n",
    "    events, _ = mne.events_from_annotations(raw, verbose=False)\n",
    "\n",
    "    # remove events from event id that are not in the data (to avoid errors when creating the epochs)\n",
    "    event_id_tmp = {key: value for key, value in event_id.items() if value in events[:, 2]}\n",
    "\n",
    "\n",
    "    # creating the epochs\n",
    "    epochs_tmp = mne.Epochs(\n",
    "        raw, \n",
    "        events, \n",
    "        event_id=event_id_tmp,\n",
    "        tmin=tmin, \n",
    "        tmax=tmax, \n",
    "        baseline=baseline, \n",
    "        preload=True, \n",
    "        reject=reject,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    # downsample the epochs\n",
    "    epochs_tmp = epochs_tmp.resample(250)\n",
    "\n",
    "    return epochs_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "data_path = Path(\"/work/EEG_lab/FaceWord_EEG\") # insert path to the preprocessed epochs of the FaceWord EEG data\n",
    "\n",
    "\n",
    "# loading in the file with the bad channels identified by each group\n",
    "session_info_path = data_path / \"session_info.txt\"\n",
    "\n",
    "    \n",
    "# load in session information (bad channels, etc.) txt file with dictionary\n",
    "with open(session_info_path, \"r\") as f:\n",
    "    session_info = eval(f.read())\n",
    "\n",
    "print(session_info)\n",
    "\n",
    "# creating a figures folder for saving plots from drop log\n",
    "fig_path = Path(\"/work/LauraBockPaulsen#1941/CogNeuro2025/EEG_LAB/figures\")\n",
    "\n",
    "if not fig_path.exists():\n",
    "    fig_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over the subjects and preprocessing\n",
    "all_epochs = []\n",
    "\n",
    "monday_recording = [\"group0-fw2\", \"group1_fw\", \"group2_fw\", \"group3_fw\", \"group4_fw\", \"group5_fw\"]\n",
    "\n",
    "for groupname, info in session_info.items():\n",
    "    print(f\"Preprocessing data from {groupname}\")\n",
    "    print(f\"Bad channels: {info[\"bad_channels\"]}\")\n",
    "\n",
    "    raw = mne.io.read_raw_brainvision(data_path / f\"{groupname}.vhdr\", preload=True)\n",
    "\n",
    "    # check if the raw needs to be cropped\n",
    "    if info[\"tmin\"] != None or info[\"tmax\"] != None: \n",
    "        raw.crop(info[\"tmin\"], info[\"tmax\"])\n",
    "    \n",
    "    if groupname in monday_recording:\n",
    "        event_ids = event_id_monday_recording\n",
    "    else:\n",
    "        event_ids = event_ids\n",
    "\n",
    "    preprocessed_epochs = preprocess_EEG_data(\n",
    "        raw, \n",
    "        bad_channels=info[\"bad_channels\"],\n",
    "        event_id = event_ids\n",
    "        )\n",
    "\n",
    "    drop_log_fig = preprocessed_epochs.plot_drop_log()\n",
    "    drop_log_fig.savefig(fig_path / f\"{groupname}.png\")\n",
    "\n",
    "    all_epochs.append(preprocessed_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to go the `05-windowed_mean_group_lvl.ipynb` to get inspiration for how to do a group-level windowed mean analysis! Add code chunks below as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for group-level windowed mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-based permutation test\n",
    "## Subject-level\n",
    "You have already loaded your data into `epochs`-variable, so you can just use that! Go to the `06-clusterbased_permutation.ipynb` to get inspiration for how to do a subject-level cluster-based permutation test! Add code chunks below as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for subject-level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-level\n",
    "\n",
    "Since you already preprocessed the data, you can just use the `all_epochs`-variable defined when preparing for the group-level windowed mean analysis. Go to the `06-clusterbased_permutation.ipynb` to get inspiration for how to do a group-level cluster-based permutation test! Add code chunks below as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for group-level "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
